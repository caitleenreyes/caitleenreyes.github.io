---
layout: post
title: Week 3
---

During Week 3, I reviewed the meanings behind the graph data of the UNet model outputs. I discussed with Danielle about the changes in results to the UNet model outputs (loss & graph data) as we changed the filters (channel & stride sizes) of the neural network. 

I created an UNet Journal document to record the modelâ€™s different outputs as I change the filters of the neural network and test different trained data against different datasets. As I trained the neural network with different filters, I saved a file path of the training model with a specified weight to be able to load the model without having to retrain the data.
- Documented the original neural network code as trained data against my dataset
- Changed the original neural network code by halving the channel numbers and documented the halved channels neural network code as trained data against my dataset
- Changed the original neural network code by doubling the channel numbers and documented the doubled channels neural network code as trained data against my dataset

I found that the original neural network code had the lowest loss, followed by the version with doubled channels, while the version with halved channels had the highest loss.

I received Slicer data of a small honeybee swarm to begin labeling. A small swarm has only been fully labeled a handful of times, making it a valuable contribution. By using a fully labeled small swarm as training data, the neural network can achieve a better understanding of larger swarms
